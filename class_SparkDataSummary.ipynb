{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom typing import Union"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"114d1cb2-6933-4d8b-9ab1-ec4f0688d366","inputWidgets":{},"title":"Package Imports"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nCreated on Monday, December 2, 2022 at 15:30 by Adrian Tullock <atullock@bpcs.com>\nCopyright (C) 2022, by Blueprint Technologies. All Rights Reserved.\n\"\"\"\nclass SparkDataSummary:\n    def __init__(self, df, dbName:str, tableName:str, dtypeChanges=None):\n        \"\"\"Initialization\n        :param df (dataframe): Dataframe to be profiled\n        :param dbName (str): Database name related to table being profiled\n        :param tableName (str): Name of the table being profiled\n        :param dtypeChanges (dict): Accepts dict of column-type key-value pairs\n        e.g.: {\"col1\":\"integer\", \"col2\":\"bool\"} will attempt to cast col1 and col2 as integer and boolean types, respectively.\n        \"\"\"\n        self.dbName = dbName.lower()\n        self.tableName = tableName.lower()\n        self.df = self._convert_ts_to_string(df)\n        self._summaryTypes = ('bool','numeric','string')\n        \n        if dtypeChanges is not None:\n            self.update_dtypes(dtypeChanges)\n        \n        # Get typed dataframes\n        self._boolean_df = self._typed_df(\"bool\")\n        self._numeric_df = self._typed_df(\"numeric\")\n        self._string_df = self._typed_df(\"string\")\n        \n        # Count column values\n        self.row_count = df.count()\n        self._null_counts = self._get_null_counts()\n        self._bool_counts = self._get_bool_counts()\n        \n        # Summary data\n        self._summary = {\"bool\":None,\"numeric\":None,\"string\":None}\n        self._distinct_value_summary = None\n        self._string_data_profile = None\n    \n    #----- PUBLIC METHODS -----#\n    def distinct_value_summary(self, showSummary=True):\n        \"\"\"Create and display a distinct value counts summary for numeric and string columns\n        \"\"\"\n        distinct = lambda x: df.select(x).distinct().count()\n        \n        # Summarize string and numeric columns\n        df_columns = self._string_df.columns.copy()\n        df_columns.extend(self._numeric_df.columns)\n        df = self.df.select(df_columns)\n        \n        summary_df = df.summary('count')\n        row_template = summary_df.collect()[0].asDict()\n        \n        new_rows = []\n        new_row = row_template.copy()\n        new_row.update({'summary':'distinct'})\n        __ = [new_row.update({c:str(distinct(c))}) for c in df_columns]\n        new_rows.append(new_row)\n        \n        # Join column summary metrics\n        summary_df = summary_df.unionByName(spark.createDataFrame(new_rows))\n        summary_pivot_df = summary_df.unpivot(\"summary\", df_columns,\"ColumnName\",\"value\")\n        counts_df = summary_pivot_df.where(\n            \"summary=='count'\").withColumnRenamed(\n            \"value\",\"TotalValues\").drop(\"summary\")\n        \n        distincts_df = summary_pivot_df.where(\n            \"summary=='distinct'\").withColumnRenamed(\n            \"value\",\"DistinctCount\").withColumnRenamed(\n            \"ColumnName\",\"Match\").drop(\"summary\")\n        summary_pivot_df = counts_df.join(distincts_df, [counts_df.ColumnName==distincts_df.Match]).drop(\"Match\")\n        \n        final_summary_df = summary_pivot_df.withColumn(\n            \"SourceDatabase\",lit(self.dbName)).withColumn(\n            \"SourceTable\",lit(self.tableName)).withColumn(\n            \"TotalRecords\",lit(self.row_count).astype(LongType()))\n        \n        final_summary_df = final_summary_df.select(\n            \"SourceDatabase\",\"SourceTable\",\"TotalRecords\",\"ColumnName\",col(\"TotalValues\").astype(LongType()),\n            col(\"DistinctCount\").astype(LongType())).orderBy(\n            col(\"DistinctCount\").desc())\n        \n        self._distinct_value_summary = final_summary_df\n        \n        if showSummary:\n            print(\"Distinct Values Summary:\")\n            display(self._distinct_value_summary)\n        \n        return None\n        \n    def full_summary(self, showSummary=True):\n        \"\"\"Create and display all available summary data\"\"\"\n        self.summarize('numeric', showSummary)\n        self.summarize('string', showSummary)\n        self.summarize('bool', showSummary)\n        self.distinct_value_summary(showSummary)\n        \n        return None\n    \n    def get_distinct_value_summary(self):\n        \"\"\"Retrieve distinct value data summary\n        \"\"\"\n        summary = self._distinct_value_summary\n        \n        if summary is None:\n            self.distinct_value_summary(showSummary=False)\n            summary = self._distinct_value_summary\n        \n        return summary\n    \n    def get_summary(self, dtype):\n        \"\"\"Retrieve a typed data summary\n        :param dtype (str): Accepts these values: \"bool\", \"numeric\", \"string\"\n        \"\"\"\n        assert dtype in self._summaryTypes, f\"Invalid dtype: accepts {self._summaryTypes}\"\n        summary = self._summary.get(dtype)\n        \n        if summary is None:\n            self.summarize(dtype, showSummary=False)\n            summary = self._summary.get(dtype)\n        \n        return summary\n    \n    def get_unique_string_values_summary(self):\n        \"\"\"Retrieve unique string values summary\n        \"\"\"\n        summary = self._string_data_profile\n        \n        if summary is None:\n            self.unique_string_values_count(showSummary=False)\n            summary = self._string_data_profile\n        \n        return summary\n    \n    def summarize(self, dtype='numeric', showSummary=True):\n        \"\"\"Create and display a profile for all columns of the specified 'dtype'\n        :param dtype (str): Accepts these values: \"bool\", \"numeric\", \"string\"\n        \"\"\"\n        assert dtype in self._summaryTypes, f\"Invalid dtype: accepts {self._summaryTypes}\"\n        \n        if self._summary.get(dtype) is None:\n            df = {\n                \"bool\":self._boolean_df,\n                \"numeric\":self._numeric_df,\n                \"string\":self._string_df\n            }[dtype.lower()]\n\n            row_template = df.summary('count').collect()[0].asDict()\n\n            if len(row_template.keys()) < 2:\n                # Manually make a row template\n                row_template = self._make_row_template(df.columns)\n\n            # Get df columns\n            columns = [x for x in row_template.keys()][1:]\n\n            new_rows = []\n\n            if dtype == 'bool':\n                # Add rows for true/false counts\n                new_rows.append(self._bool_counts.get(\"ALL\"))\n                new_rows.append(self._bool_counts.get(\"T\"))\n                new_rows.append(self._bool_counts.get(\"F\"))\n\n            if dtype == 'string':\n                # Add string column metadata\n                new_row = self._create_new_row('distinct', row_template, rowType='distinct')\n                new_rows.append(new_row)\n\n            # Add row for null counts\n            new_row = self._create_new_row('nulls', row_template)\n            new_rows.append(new_row)\n\n            # Add row for null %\n            new_row = self._create_new_row('null%', row_template, rowType='percent')\n            new_rows.append(new_row)\n\n            if dtype == 'bool':\n                summary_df = spark.createDataFrame(new_rows).select(list(row_template.keys()))\n            elif dtype in ('numeric'):\n                summary_df = df.summary()\n                summary_df = summary_df.unionByName(spark.createDataFrame(new_rows))\n            elif dtype == 'string':\n                summary_df = df.summary('count')\n                summary_df = summary_df.unionByName(spark.createDataFrame(new_rows))\n                \n            self._summary.update({dtype:summary_df})\n            \n        if showSummary:\n            print(f\"{dtype.title()} Data Summary:\")\n            display(self._summary.get(dtype))\n        \n        return None\n    \n    def unique_string_values_count(self, showSummary=False, returnProfile=False, exclude: Union[str, list]=None):\n        \"\"\"Identify and count unique values for each string column\"\"\"\n        retValue = None\n        if self._string_data_profile is None:\n            strSummary = self.get_summary('string')\n            str_columns = strSummary.columns[1:]\n            \n            # Create dataframe\n            schema = self._get_summary_schema('unique_string_values')\n            dtls_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n            \n            # Exclude some columns\n            if exclude is not None:\n                if type(exclude) is str:\n                    try:\n                        str_columns.remove(exclude)\n                    except:\n                        pass\n                if type(exclude) is list:\n                    for x in exclude:\n                        try:\n                            str_columns.remove(x)\n                        except:\n                            pass\n                    \n            # Extract string column values data\n            for column in str_columns:\n                col_df = self.df.select(column).withColumn('CountCol', lit(1))\n                valueCounts = col_df.groupby(column).sum().collect()\n                \n                new_rows = []\n                for vc in valueCounts:\n                    v0 = vc[0]\n                    v1 = vc[1]\n                    new_rows.append((self.dbName, self.tableName, column, v0, v1))\n                    \n                dtls_df = dtls_df.union(spark.createDataFrame(new_rows, schema))\n                \n            self._string_data_profile = dtls_df\n            \n        if returnProfile:\n            retValue = self._string_data_profile\n            \n        if showSummary:\n            print(\"Unique String Values Summary:\")\n            display(self._string_data_profile)\n        \n        return retValue\n    \n    def update_dtypes(self, colTypes:str, returnSchema=True):\n        \"\"\"Cast columns as different types\n        :param colTypes (dict): Accepts dict of column-type key-value pairs\n        e.g.: \"col1 int,col2 bool\" will attempt to cast col1 and col2 as integer and boolean types, respectively.\n        \n        :param returnSchema (boolean): Returns new dataframe schema, if True\n        \"\"\"\n        for column,dtype in zip(colTypes.keys(),colTypes.values()):\n            sparkType = self._get_spark_data_type(dtype)\n            try:\n                self.df = self.df.withColumn(column, col(column).cast(sparkType))\n            except Exception as e:\n                print(f\"Unable to cast column {column} of type {dtype} to {sparkType} \\n{e}\")\n        \n        # Reassemble the typed dataframes\n        self._boolean_df = self._typed_df(\"bool\")\n        self._numeric_df = self._typed_df(\"numeric\")\n        self._string_df = self._typed_df(\"string\")\n        self._bool_counts = self._get_bool_counts()\n        \n        # Reset data summary\n        self._summary = {\"bool\":None,\"numeric\":None,\"string\":None}\n        \n        if returnSchema:\n            retValue = self.df.schema\n        else:\n            retValue = None\n        \n        return retValue\n    \n    def write_all_summary_data(self, profileDB='data_profile'):\n        \"\"\"Write all summary data to the database\n        :param profileDB (str): Database to write profile data to\n        \"\"\"\n        # Write typed data summaries\n        for dtype in self._summaryTypes:\n            self.write_type_summary(dtype, profileDB)\n        \n        # Write distinct value summary\n        self.write_distinct_value_summary(profileDB)\n        \n        return None\n    \n    def write_distinct_value_summary(self, profileDB='data_profile'):\n        \"\"\"Write distinct value summary to the database\n        :param profileDB (str): Database to write profile data to\n        \"\"\"\n        summary_data = self._distinct_value_summary\n        summary_type = \"distinct_values\"\n        \n        if self.row_count < 1:\n            print(f\"No data found in {self.tableName}. Skipping distinct value summary.\")\n        else:\n            print(f\"Attempting to write distinct value summary for {self.tableName} . . .\")\n            \n            # Collect metadata rows\n            summary_df = summary_data.withColumn(\"__date\", current_date())\n            summary_df = self._transform_summary(summary_df, summary_type)\n            \n            # Write summary to database\n            self._summary_writer(summary_df, summary_type, profileDB)\n        return None\n    \n    def write_type_summary(self, dtype:str, profileDB='data_profile'):\n        \"\"\"Write typed summary data to the database\n        :param dtype (str): Data type summary to save\n        :param profileDB (str): Database to write profile data to\n        \"\"\"\n        assert dtype.lower() in self._summaryTypes, f\"Invalid dtype: accepts {self._summaryTypes}\"\n        summary_data = self._summary.get(dtype)\n        \n        if len(summary_data.columns) < 2:\n            print(f\"No {dtype} columns found in {self.tableName}. Skipping . . .\")\n        else:\n            print(f\"Attempting to write {dtype} summary for {self.tableName} . . .\")\n            \n            # Collect metadata rows\n            new_rows_list = []\n            for c in summary_data.columns[1:]:\n                row_values = [self.dbName,self.tableName,c]\n                _ = [row_values.append(x.asDict().get(c)) for x in summary_data.select(c).collect()]\n                new_rows_list.append(row_values)\n                \n            columns = ['SourceDatabase','SourceTable','ColumnName']\n            _ = [columns.append(x.asDict().get('summary').title()) for x in summary_data.select(\"summary\").collect()]\n            \n            summary_df = spark.createDataFrame(new_rows_list, columns).withColumn(\"__date\", current_date())\n            summary_df = self._transform_summary(summary_df,dtype)\n            \n            # Write summary to database\n            self._summary_writer(summary_df, dtype, profileDB)\n        return None\n    \n    def write_unique_string_values_summary(self, profileDB='data_profile', exclude: Union[str, list]=None):\n        \"\"\"Write unique string value summary to the database\n        :param profileDB (str): Database to write profile data to\n        \"\"\"\n        if self._string_data_profile is None:\n            self.unique_string_values_count(exclude=exclude)\n        \n        summary_data = self._string_data_profile\n        summary_type = \"unique_string_values\"\n        \n        if summary_data is None:\n            print(f\"No string data found in {self.tableName}. Skipping unique string values summary.\")\n        else:\n            print(f\"Attempting to write unique string values summary for {self.tableName} . . .\")\n            \n            # Collect metadata rows\n            summary_df = summary_data.withColumn(\"__date\", current_date().astype(DateType()))\n            \n            # Write summary to database\n            self._summary_writer(summary_df, summary_type, profileDB)\n        return None\n    \n    #----- PRIVATE METHODS -----#\n    @classmethod\n    def _convert_ts_to_string(cls, df):\n        \"\"\"Helper function to convert timestamp columns into string columns\"\"\"\n        # Cast timestamp columns as string values\n        for column in [c[0] for c in df.dtypes if c[1] == 'timestamp']:\n            df = df.withColumn(column, col(column).cast(StringType()))\n            \n        return df\n    \n    def _create_new_row(self, title:str, rowTemplate:dict, rowType:str='count'):\n        \"\"\"Helper function to create a new row entry\n        :param title: Title for the metadata row\n        :param rowTemplate: Template containing the columns to compute calculations for\n        :param rowType: Accepts values in (count, percent, distinct)\n        \"\"\"\n        columns = [x for x in rowTemplate.keys()][1:]\n        new_row_dict = rowTemplate.copy()\n        new_row_dict.update({'summary':title})\n        if rowType == 'count':\n            __ = [new_row_dict.update({c:str(self._null_counts.get(c))}) for c in columns]\n        elif rowType == 'percent':\n                __ = [new_row_dict.update({c:str(100*(self._null_counts.get(c)/self.row_count))}) for c in columns]\n        elif rowType == 'distinct':\n            distinct = lambda x: self._string_df.select(x).distinct().count()\n            __ = [new_row_dict.update({c:str(distinct(c))}) for c in columns]\n        \n        return new_row_dict\n    \n    def _get_bool_counts(self):\n        \"\"\"Get the columnwise count for boolean values\n        \"\"\"\n        true_counter = lambda c: self._boolean_df.where(f\"{c} is true\").count()\n        false_counter = lambda c: self._boolean_df.where(f\"{c} is false\").count()\n        true_counts = {'summary':\"true\"}\n        false_counts = {'summary':\"false\"}\n        value_counts = {'summary':\"count\"}\n        \n        for name in [x.name for x in self._boolean_df.schema]:\n            t_count = true_counter(name)\n            f_count = false_counter(name)\n            value_count = t_count + f_count\n            true_counts.update({name:str(t_count)})\n            false_counts.update({name:str(f_count)})\n            value_counts.update({name:str(value_count)})\n            \n        bool_counts = {'T':true_counts, 'F':false_counts, 'ALL':value_counts}\n        return bool_counts\n    \n    def _get_null_counts(self):\n        \"\"\"Get the columnwise null count\n        \"\"\"\n        null_counter = lambda c: self.df.where(f\"{c} is null\").count()\n        null_counts = {}\n        for name in [x.name for x in self.df.schema]:\n            null_counts.update({name:null_counter(name)})\n            \n        return null_counts\n    \n    @classmethod\n    def _get_spark_data_type(cls, typeName:str):\n        \"\"\"Identify a column's equivalent Spark data type\"\"\"\n        binaryType = lambda x: any([t in x.lower() for t in ['bit']])\n        boolType = lambda x: any([t in x.lower() for t in ['bool']])\n        decimalType = lambda x: any([t in x.lower() for t in ['decimal','numeric']])\n        doubleType = lambda x: any([t in x.lower() for t in ['double','money']])\n        floatType = lambda x: any([t in x.lower() for t in ['float']])\n        intType = lambda x: any([t in x.lower() for t in ['int','long']])\n        strType = lambda x: any([t in x.lower() for t in ['char','date','string','time']])\n\n        if binaryType(typeName):\n            dataType = BinaryType()\n        elif boolType(typeName):\n            dataType = BooleanType()\n        elif decimalType(typeName):\n            dataType = DecimalType()\n        elif doubleType(typeName):\n            dataType = DoubleType()\n        elif floatType(typeName):\n            dataType = FloatType()\n        elif intType(typeName):\n            dataType = LongType()\n        elif strType(typeName):\n            dataType = StringType()\n        else:\n            dataType = StringType()\n\n        return dataType\n    \n    @classmethod\n    def _get_summary_schema(cls, schemaType):\n        \"\"\"Retrieve schema for summary data\n        NOTE: timestamp type not yet implemented\n        \"\"\"\n        schema = {'bool':\n                  StructType([\n                      StructField('SourceDatabase', StringType()),\n                      StructField('SourceTable', StringType()),\n                      StructField('ColumnName', StringType()),\n                      StructField('Count', LongType()),\n                      StructField('True', LongType()),\n                      StructField('False', LongType()),\n                      StructField('Nulls', LongType()),\n                      StructField('Null%', DecimalType(5,2)),\n                      StructField('__date', DateType())]),\n                  'distinct_values':\n                  StructType([\n                      StructField('SourceDatabase', StringType()),\n                      StructField('SourceTable', StringType()),\n                      StructField('TotalRecords', LongType()),\n                      StructField('ColumnName', StringType()),\n                      StructField('TotalValues', LongType()),\n                      StructField('DistinctCount', LongType()),\n                      StructField('__date', DateType())]),\n                  'numeric':\n                  StructType([\n                      StructField('SourceDatabase', StringType()),\n                      StructField('SourceTable', StringType()),\n                      StructField('ColumnName', StringType()),\n                      StructField('Nulls', LongType()),\n                      StructField('Null%', DecimalType(5,2)),\n                      StructField('Count', LongType()),\n                      StructField('Mean', FloatType()),\n                      StructField('Stddev', FloatType()),\n                      StructField('Min', FloatType()),\n                      StructField('25%', FloatType()),\n                      StructField('50%', FloatType()),\n                      StructField('75%', FloatType()),\n                      StructField('Max', FloatType()),\n                      StructField('__date', DateType())]),\n                  'string':\n                  StructType([\n                      StructField('SourceDatabase', StringType()),\n                      StructField('SourceTable', StringType()),\n                      StructField('ColumnName', StringType()),\n                      StructField('Distinct', LongType()),\n                      StructField('Nulls', LongType()),\n                      StructField('Null%', DecimalType(5,2)),\n                      StructField('Count', LongType()),\n                      StructField('__date', DateType())]),\n                  'timestamp':\n                  StructType([\n                      StructField('SourceDatabase', StringType()),\n                      StructField('SourceTable', StringType()),\n                      StructField('ColumnName', StringType()),\n                      StructField('Distinct', LongType()),\n                      StructField('Nulls', LongType()),\n                      StructField('Null%', DecimalType(5,2)),\n                      StructField('Count', LongType()),\n                      StructField('__date', DateType())]),\n                  'unique_string_values':\n                  StructType([\n                      StructField('SourceDatabase', StringType(), False),\n                      StructField('SourceTable', StringType(), False),\n                      StructField('ColumnName', StringType(), False),\n                      StructField(\"ColumnValue\", StringType(), True),\n                      StructField(\"DistinctCount\", IntegerType(), True)])\n                 }[schemaType]\n        \n        return schema\n    \n    @classmethod\n    def _make_row_template(cls, columns):\n        \"\"\"Helper function to manually create a row template\n        :param columns (list): Column names to include in the summary\"\"\"\n        template = {'summary':\"\"}\n        __ = [template.update({c:\"\"}) for c in columns]\n        \n        return template\n    \n    @classmethod\n    def _summary_writer(cls, df, dtype, profileDB):\n        \"\"\"Helper function to handle writing summaries to the database\n        :param df (str): Dataframe carrying the profile data\n        :param dtype (str): Data type summary to save\n        :param profileDB (str): Database to write profile data to\n        \"\"\"\n        if not spark.catalog.databaseExists(profileDB):\n            print(f\"Creating database: {profileDB}\")\n            spark.sql(f\"CREATE DATABASE {profileDB}\")\n            \n        spark.catalog.setCurrentDatabase(profileDB)\n        table = dtype.lower() + \"_summary\"\n        \n        df.write.saveAsTable(table, mode='append', partitionBy=['__date','SourceDatabase','SourceTable'])\n        _ = spark.sql(f\"OPTIMIZE {profileDB}.{table}\")\n        print(\"Write successful!\")\n        return None\n    \n    @classmethod\n    def _transform_summary(cls, df, schemaType):\n        \"\"\"Function for conforming summary data to their proper schemas\"\"\"\n        schema = cls._get_summary_schema(schemaType)\n        \n        for field in schema:\n            name = field.name\n            dtype = field.dataType\n            df = df.withColumn(name, col(name).cast(dtype))\n            \n        return df\n    \n    def _typed_df(self, dtype):\n        \"\"\"Subset a dataframe by column data types\n        \"\"\"\n        assert dtype.lower() in self._summaryTypes, f\"Invalid dtype: accepts {self._summaryTypes}\"\n        getType = lambda t: str(t).split(\"Type\")[0].lower()\n        \n        dtypes = {\n            'bool':('boolean'),\n            'numeric':('binary', 'decimal', 'double', 'float', 'integer', 'long'),\n            'string':('string', 'timestamp')\n        }[dtype.lower()]\n            \n        # Dataframe filtered to typed columns\n        fields = [f.name for f in self.df.schema if getType(f.dataType) in dtypes]\n        \n        return self.df.select(fields)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"8878d830-9a35-4c3f-a121-157975ce8178","inputWidgets":{},"title":"Class Definition"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class SummaryViewer:\n    def __init__(self, sourceTable:str, profile_db: str='data_profile'):\n        self.profile_db = profile_db\n        self._profiles = {}\n        self._collect_profiles(sourceTable)\n        \n    #----- PUBLIC METHODS-----#\n    def distinct_values_summary(self, returnProfile: bool=False):\n        if returnProfile:\n            retValue = self._profiles.get('distinct_values')\n        else:\n            print(\"Distinct Values Summary: \\n\")\n            display(self._profiles.get('distinct_values'))\n            retValue = None\n        \n        return retValue\n    \n    def numeric_summary(self, returnProfile: bool=False):\n        if returnProfile:\n            retValue = self._profiles.get('numeric')\n        else:\n            print(\"Numeric Values Summary: \\n\")\n            display(self._profiles.get('numeric'))\n            retValue = None\n        \n        return retValue\n    \n    def string_summary(self, returnProfile: bool=False):\n        if returnProfile:\n            retValue = self._profiles.get('string')\n        else:\n            print(\"String Values Summary: \\n\")\n            display(self._profiles.get('string'))\n            retValue = None\n        \n        return retValue\n    \n    def unique_string_values_summary(self, columnsFilter: Union[str, list]=None, returnProfile: bool=False):\n        profile = self._profiles.get('unique_string_values')\n        if type(columnsFilter) == str:\n            profile = profile.filter(col('ColumnName').isin(columnsFilter.upper()))\n        elif type(columnsFilter) == list:\n            columns_upper = [c.upper() for c in columnsFilter]\n            profile = profile.filter(col('ColumnName').isin(columns_upper))\n            \n        if returnProfile:\n            retValue = profile\n        else:\n            print(\"Unique String Values Summary: \\n\")\n            display(profile)\n            retValue = None\n        \n        return retValue\n    \n    def full_profile(self):\n        self.numeric_summary()\n        self.string_summary()\n        self.distinct_values_summary()\n        return None\n    \n    #----- PRIVATE METHODS-----#\n    def _collect_profiles(self, sourceTable:str):\n        build_query = lambda stype: f\"SELECT * FROM {self.profile_db}.{stype}_summary WHERE sourcetable = '{sourceTable}'\"\n        \n        summary_types = ['numeric', 'string', 'distinct_values', 'unique_string_values']\n        for st in summary_types:\n            try:\n                summary_df = spark.sql(build_query(st)).drop('__date')\n                if st == summary_types[3]:\n                    summary_df = summary_df.orderBy(['ColumnName','DistinctCount'])\n                self._profiles.update({st:summary_df})\n            except:\n                pass\n            \n        return None"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"141d5044-d156-4b96-ac76-b78fa0f06e15","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"class_SparkDataSummary","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3436385825399147}},"nbformat":4,"nbformat_minor":0}
