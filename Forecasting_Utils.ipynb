{"cells":[{"cell_type":"code","source":["from abc import ABC, abstractmethod\n\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nfrom typing import Union\n\nimport mlflow\nimport numpy as np\nimport pandas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"87e2dee6-ea33-4487-a37c-41da10f8cf64","inputWidgets":{},"title":"Import Libraries"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class SeriesData:\n    \"\"\"This class is designed to enable consistent and efficient retrieval of standard price time series data\"\"\"\n    \n    # Columns to join with after calculating moving averages\n    _remaining_cols = ['swire_period_date','unit_sales','is_promotion','promotional_price','dollar_sales',\n                        'any_price_decrease_dollar_sales','standard_physical_volume',\n                        'any_price_decrease_standard_physical_volume','state']\n    \n    # Column lists for ordering data and partitioning windows\n    _df_order_cols = ['period_number','pod_id','product_key','key_brand','key_package']\n    _window_order_cols = ['pod_id','product_key','period_number']\n    _window_partition_cols = ['pod_id','product_key','key_brand','key_package']\n    \n    # Variables for assisting with consistency\n    DATABASE = 'stf_db'\n    TABLE = f'{DATABASE}.product_sales'  # Table containing sales data\n    STD_PRICE = 'standard_price'\n    _TRANS_NUM = 'transaction_number'\n    SERIES_TABLE = f'{DATABASE}.product_sales_series'\n        \n    #----- PUBLIC METHODS -----#\n    @classmethod\n    def calc_dollar_promotion(cls, df):\n        '''\n        Function to calculate the dollars spent on promotion\n        '''\n        dollar_promo = lambda d: f'dollar_promo_{d}wk'\n        week_number = lambda w: int(w.rsplit('_', maxsplit=1)[1].rstrip('wk'))\n        \n        avg_sp_cols_list = [c for c in df.columns if c.find('wk') >=0]\n        wk_windows_list = [week_number(w) for w in avg_sp_cols_list]\n        \n        for w,c in zip(wk_windows_list, avg_sp_cols_list):\n            col_name = dollar_promo(w)\n            df = df.withColumn(col_name, \\\n                           col('avg_unit_sales') * (col(c) - col('avg_promotional_price'))) \\\n            .withColumn(col_name, round(when(col(col_name)<0, 0).otherwise(col(col_name)),2))\n\n        return df\n    \n    \n    @staticmethod\n    def estimate_physical_volume_range(df):\n        \"\"\"\n        df: Spark dataframe containing swire_period date and standard_physical_volume columns\n        \"\"\"\n        volume_by_period = [x.asDict().get('sum(standard_physical_volume)') for x in \\\n                            df.groupBy('swire_period_date').sum('standard_physical_volume').collect()]\n        avg_spv = np.mean(volume_by_period)\n        med_spv = np.median(volume_by_period)\n        stdDev_spv = np.std(volume_by_period)\n        \n        print(f\"Average Physical Volume: {np.round(avg_spv,2)}\\nMedian Physical Volume: {np.round(med_spv,2)}\\nStandard Deviation: {np.round(stdDev_spv,2)}\\n\")\n        print(f\"Physical Volume Range Expectation: {np.round(avg_spv - stdDev_spv,2)} to {np.round(avg_spv + stdDev_spv,2)}\\n\")\n        return None\n    \n    @classmethod\n    def group_by_state_and_keyCol(cls, keyCol: str='key_brand'):\n        \"\"\"\n        Retrieve a dataframe with metric values grouped by state and one additional parameter\n        - Parameters -\n        keyCol: Accepts the following values: 'key_brand','key_package'\n        \"\"\"\n        group_cols = ['state','period_number', 'swire_period_date']\n        group_cols.insert(0, keyCol)\n        \n        # Compute grouped averages\n        df = spark.sql(f\"SELECT * FROM {cls.SERIES_TABLE}\")\n        value_cols = [vc[0] for vc in df.dtypes if vc[1].find('double') >= 0]\n        df = df.groupBy(group_cols).mean(*value_cols).orderBy(group_cols)\n        \n        # Round values to 2 decimal places\n        avg_value_cols = [f'avg({vc})' for vc in value_cols]\n        for vc, avc in zip(value_cols, avg_value_cols):\n            df = df.withColumn(avc, (round(avc,2))).withColumnRenamed(f'avg({vc})', f'avg_{vc}')\n            \n        return df\n    \n    @classmethod\n    def std_price_moving_average(cls, numWeeksWindow: Union[int, list]=0):\n        \"\"\"\n        Retrieve sales data with standard price moving averages computed\n        - Parameters -\n        numWeeksWindow: The number of weeks to use for the moving average window. Accepts int or list types.\n        \"\"\"\n        # Retrieve standard_price data\n        select_cols = [cls._TRANS_NUM,'period_number','pod_id','product_key','key_brand','key_package', cls.STD_PRICE]\n        query = f\"SELECT {', '.join(select_cols)} FROM {cls.TABLE} ORDER BY {', '.join(cls._df_order_cols)}\"\n        df_std_price = spark.sql(query)\n        \n        # Impute missing standard_price values\n        df = cls._fill_standard_price(df_std_price)\n        \n        def compute_moving_average(cls, df, numWeeks: int):\n            if numWeeks > 0:\n                lag_price_str = 'lag_price'\n                \n                # Generate lag_price columns\n                lag_columns = [f\"{lag_price_str}{x+1}\" for x in range(numWeeks)]\n                prev_columns = [cls.STD_PRICE]\n                lag_position = [x+1 for x in range(numWeeks)]\n                _ = [prev_columns.append(f\"{lag_price_str}{x+1}\") for x in range(numWeeks-1)]\n                \n                # Create moving average column\n                Window_spec = Window.partitionBy(cls._window_partition_cols).orderBy(cls._window_order_cols)\n                lag_window = lambda x: lag(cls.STD_PRICE, x).over(Window_spec)\n                mvgAvg_col = f\"{cls.STD_PRICE}_{numWeeks}wk\"\n                df = df.withColumn(mvgAvg_col, lit(0))\n                \n                # Compute moving average\n                for lag_c, position, prev_c in zip(lag_columns, lag_position, prev_columns):\n                    \n                    df= df.withColumn(lag_c, when(lag_window(position).isNotNull(), lag_window(position)).otherwise(col(prev_c)))\n                    df = df.withColumn(mvgAvg_col, col(mvgAvg_col)+col(lag_c))\n                    \n                for lag_c in lag_columns:\n                    df = df.drop(lag_c)\n                    \n                df = df.withColumn(mvgAvg_col, round((col(mvgAvg_col)/numWeeks).astype(DoubleType()),2))\n                \n            return df\n        \n        if type (numWeeksWindow) is int:\n            df = compute_moving_average(cls, df, numWeeksWindow)\n        elif type (numWeeksWindow) is list:\n            for w in numWeeksWindow:\n                df = compute_moving_average(cls, df, w)\n        \n        # Retrieve and join remaining table columns\n        df = cls._get_remaining_columns(df)\n        \n        # Order results\n        df = df.orderBy(cls._window_order_cols)\n        \n        return df\n    \n    @classmethod\n    def time_series_transform_by_keyCols(cls, df, keyCol1: str='state', keyCol2: str='key_brand'):\n        \"\"\"\n        Function to extract final time series\n        - Parameters:\n        df: Spark Dataframe\n        keyCol1 (str): name of the first key column for the target timeseries\n        keyCol2 (str): name of the second key column for the target timeseries\n        \"\"\"\n        promo_cols = [c for c in df.columns if c.find('dollar_promo') >= 0]\n        order_cols = [keyCol1, keyCol2, 'period_number','swire_period_date']\n        select_cols = order_cols + ['avg_standard_physical_volume'] + promo_cols\n        \n        timeSeries_df = df.select(select_cols).orderBy(order_cols)\n\n        return timeSeries_df\n    \n    @classmethod\n    def write_moving_avg_table(cls, df, tableName: str='product_sales_series'):\n        series_table = f\"{cls.DATABASE}.product_sales_series\"\n        _ = spark.sql(f'DROP TABLE IF EXISTS {series_table}')\n        result = mvg_avg_data.write.saveAsTable(series_table, format='delta', mode='overwrite')\n        _ = spark.sql(f\"OPTIMIZE {series_table}\")\n        \n        return result\n    \n    @classmethod\n    def write_to_delta_table(cls, df, tableName):\n        \"\"\"\n        function to write the input dataframe to a delta table\n        - Parameters:\n        df: Spark Dataframe\n        tableName (str): name of the output delta table \n        \"\"\"\n        table = f\"{cls.DATABASE}.{tableName}\"\n        _ = spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n        result = df.write.saveAsTable(table, format='delta', mode='overwrite')\n        _ = spark.sql(f\"OPTIMIZE {table}\")\n        \n        return result\n    \n    #----- PRIVATE METHODS -----#\n    @classmethod\n    def _fill_standard_price(cls, df):\n        \"\"\"\n        Fill in missing standard price gaps with the values closest to them\n        \"\"\"\n        Window_spec = Window.partitionBy(cls._window_partition_cols).orderBy(cls._window_order_cols)\n        read_last = last(df[cls.STD_PRICE], ignorenulls=True).over(Window_spec)\n        df = df.withColumn(cls.STD_PRICE, read_last)\n        \n        window_order_cols_r = [col(c).desc() for c in cls._window_order_cols]\n        Window_spec_r = Window.partitionBy(cls._window_partition_cols).orderBy(window_order_cols_r)\n\n        df_reverse = df.orderBy(cls._df_order_cols, ascending=False)\n        read_last_r = last(df_reverse[cls.STD_PRICE], ignorenulls=True).over(Window_spec_r)\n        df_reverse = df_reverse.withColumn(cls.STD_PRICE, read_last_r)\n        df = df_reverse.orderBy(cls._df_order_cols, ascending=True)\n        \n        return df\n    \n    @classmethod\n    def _get_remaining_columns(cls, df):\n        \"\"\"\n        Append additional sales data columns to the standard price data\n        \"\"\"\n        select_cols = cls._remaining_cols.copy()\n        select_cols.insert(0,cls._TRANS_NUM)\n        \n        query = f\"SELECT {', '.join(select_cols)} FROM {cls.TABLE}\"\n        df_remaining = spark.sql(query)\n        \n        full_df = df.join(df_remaining, cls._TRANS_NUM, how='left').drop(cls._TRANS_NUM)\n        return full_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"96499e1d-72e8-4298-9881-c74c82449374","inputWidgets":{},"title":"Series Data Class Definition"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class SeriesModel(ABC):\n    \"\"\"\n    This class is designed to assist with modeling the time series data\n    \"\"\"\n    DATABASE = 'stf_db'\n    \n    #----- PUBLIC METHODS -----#\n    @classmethod\n    def append_time_features(cls, df):\n        sp_date = df.swire_period_date\n        df = df.withColumn('weekofyear',weekofyear(sp_date)).\\\n            withColumn('month',month(sp_date)).\\\n            withColumn('quarter',quarter(sp_date)).\\\n            withColumn('year',year(sp_date))\n        \n        return df\n    \n    @abstractmethod\n    def get_filtered_series(self, state: str, keyBrand: str):\n        pass\n    \n    @classmethod\n    def get_model_name(cls, state: str, keyVal: str):\n        model_name = f\"{state}_{keyVal}\".replace(\" \", \"_\").replace(\".\", \"_\")\n        return model_name\n    \n    @abstractmethod\n    def get_swire_period_date(self, periodNumber: int, date_format :str='%m-%d-%Y'):\n        pass\n    \n    @abstractmethod\n    def predict(self, model_params_dict: dict, state: str, keyCol: str):\n        pass\n    \n    @abstractmethod\n    def quick_stats(self):\n        pass\n    \n    #----- PRIVATE METHODS -----#\n    @abstractmethod\n    def _get_period_data(self):\n        pass\n    \n    def _predict(self, model_params_dict: dict, state: str, keyCol: str, keyType: str, evalMetric: str='r2'):\n        \"\"\"\n        This function runs a forecasting model for predicting physical volume.\n        - Parameters:\n        model_params_dict: A dict containing independent variable parameters for the prediction\n        state: The value of the state dimension to filter on\n        keyCol: The value of the product dimension (brand name or package type) to filter on\n        keyType: The type of model to search for (e.g.: 'keyBrand' or 'keyPackage')\n        evalMetric: The metric used to evaluate the best model. Assumes higher metric values are better.\n        \"\"\"\n        run_name_col = 'run_name'\n        mlflow_metrics = ['mae','rmse','mse','r2']\n        metric_cols = [f\"metrics.{m}\" for m in mlflow_metrics]\n        run_cols = ['run_id',run_name_col]\n        _ = [run_cols.append(m) for m in metric_cols]\n        eval_metric = [mc for mc in metric_cols if evalMetric in mc].pop()\n        \n        experiments = mlflow.MlflowClient().search_experiments()\n        experiment_ids = [dict(x).get('experiment_id') for x in experiments if dict(x).get('name').find(keyType) >=0]\n        \n        # Retrieve model run ID\n        runs_df = mlflow.search_runs(experiment_ids, order_by=[\"tags.mlflow.runName ASC\"]).\\\n                rename({'tags.mlflow.runName':run_name_col}, axis=1)[run_cols]\n        \n        model_name = kbm.get_model_name(state, keyCol)\n        run_id = runs_df[runs_df[run_name_col] == model_name].sort_values(eval_metric)['run_id'].values[0]\n        \n        # Load model as a PyFuncModel\n        model_uri = f'runs:/{run_id}/SparkML-linear-regression'\n        model = mlflow.pyfunc.load_model(model_uri)\n        \n        # Predict on a Pandas DataFrame\n        results = model.predict(pd.DataFrame(model_params_dict))\n        \n        return results\n    \n    @abstractmethod\n    def _retrieve_data(self):\n        pass\n\nclass KeyBrandModel(SeriesModel):\n    def __init__(self):\n        self._key_col = 'key_brand'\n        self._period_dict = {}\n        \n        self.series_data = self._retrieve_data()\n        self.keys_list = self._get_unique_col_values(self._key_col)\n        self.period_list = self._get_period_data()\n        self.states_list = self._get_unique_col_values('state')\n        \n    #----- PUBLIC METHODS -----#\n    def get_filtered_series(self, state: str, keyBrand: str):\n        df_filter = (col('state') == state) & (col(self._key_col) == keyBrand)\n        df = self.series_data.where(df_filter)\n        return df\n    \n    def get_swire_period_date(self, periodNumber: int, date_format :str='%m-%d-%Y'):\n        return self._period_dict.get(periodNumber).strftime(date_format)\n    \n    def predict(self, model_params_dict: dict, state: str, keyCol: str, evalMetric: str='r2'):\n        keyType = 'keyBrand_2'\n        results = self._predict(model_params_dict, state, keyCol, keyType, evalMetric)\n        return results\n    \n    def quick_stats(self):\n        print(f\"{'-'*3} Key Brand Quick Stats {'-'*3}\\n{'-'*29}\")\n        \n        print(f\"• {len(self.keys_list)} unique {self._key_col.replace('_',' ')} values\")\n        print(f\"• {len(self.states_list)} unique states\")\n        print(f\"• {len(self.period_list)} unique periods\")\n        print(f\"   🠆 Min: Week ending on {self.get_swire_period_date(self.period_list[0])}\")\n        print(f\"   🠆 Max: Week ending on {self.get_swire_period_date(self.period_list[-1])}\")\n        print(\"\\n\")\n        return None\n    \n    #----- PRIVATE METHODS -----#\n    def _get_period_data(self):\n        period_number = 'period_number'\n        period_date = 'swire_period_date'\n        column_data = self.series_data.select(period_number, period_date).distinct().orderBy(period_number).collect()\n        \n        __ = [self._period_dict.update({d.asDict().get(period_number):d.asDict().get(period_date)}) for d in column_data]\n        numbers = [k for k in self._period_dict.keys()]\n        return numbers\n    \n    def _get_unique_col_values(self, colName:str):\n        column_data = self.series_data.select(colName).distinct().orderBy(colName).collect()\n        values = [v.asDict().get(colName) for v in column_data]\n        return values\n    \n    def _retrieve_data(self):\n        table = f\"{self.DATABASE}.state_{self._key_col.replace('_','')}_timeseries\"\n        data = spark.read.table(table)\n        return data\n\nclass KeyPackageModel(SeriesModel):\n    def __init__(self):\n        self._key_col = 'key_package'\n        self._period_dict = {}\n        \n        self.series_data = self._retrieve_data()\n        self.keys_list = self._get_unique_col_values(self._key_col)\n        self.period_list = self._get_period_data()\n        self.states_list = self._get_unique_col_values('state')\n        \n    #----- PUBLIC METHODS -----#\n    def get_filtered_series(self, state: str, keyPackage: str):\n        df_filter = (col('state') == state) & (col(self._key_col) == keyPackage)\n        df = self.series_data.where(df_filter)\n        return df\n    \n    def get_swire_period_date(self, periodNumber: int, date_format :str='%m-%d-%Y'):\n        return self._period_dict.get(periodNumber).strftime(date_format)\n    \n    def predict(self, model_params_dict: dict, state: str, keyCol: str, evalMetric: str='r2'):\n        keyType = 'keyPackage'\n        results = self._predict(model_params_dict, state, keyCol, keyType, evalMetric)\n        return results\n    \n    def quick_stats(self):\n        print(f\"{'-'*3} Key Package Quick Stats {'-'*3}\\n{'-'*31}\")\n        \n        print(f\"• {len(self.keys_list)} unique {self._key_col.replace('_',' ')} values\")\n        print(f\"• {len(self.states_list)} unique states\")\n        print(f\"• {len(self.period_list)} unique periods\")\n        print(f\"   🠆 Min: Week ending on {self.get_swire_period_date(self.period_list[0])}\")\n        print(f\"   🠆 Max: Week ending on {self.get_swire_period_date(self.period_list[-1])}\")\n        print(\"\\n\")\n        return None\n    \n    #----- PRIVATE METHODS -----#\n    def _get_period_data(self):\n        period_number = 'period_number'\n        period_date = 'swire_period_date'\n        column_data = self.series_data.select(period_number, period_date).distinct().orderBy(period_number).collect()\n        \n        __ = [self._period_dict.update({d.asDict().get(period_number):d.asDict().get(period_date)}) for d in column_data]\n        numbers = [k for k in self._period_dict.keys()]\n        return numbers\n    \n    def _get_unique_col_values(self, colName:str):\n        column_data = self.series_data.select(colName).distinct().orderBy(colName).collect()\n        values = [v.asDict().get(colName) for v in column_data]\n        return values\n    \n    def _retrieve_data(self):\n        table = f\"{self.DATABASE}.state_{self._key_col.replace('_','')}_timeseries\"\n        data = spark.read.table(table)\n        return data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"95d2b359-70da-4afd-abef-1c26d79601e6","inputWidgets":{},"title":"Series Models Class Definition"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Forecasting_Utils","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":853712951998220}},"nbformat":4,"nbformat_minor":0}
